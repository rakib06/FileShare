{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rakib06/FileShare/blob/main/train_LIVENESS_MODEL_30_AUG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "76mcGZTxN_fE",
        "outputId": "67f837bd-7485-4498-f52a-bb683cbe888c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'face-recognition-with-liveness-web-login'...\n",
            "remote: Enumerating objects: 173, done.\u001b[K\n",
            "remote: Counting objects: 100% (108/108), done.\u001b[K\n",
            "remote: Compressing objects: 100% (95/95), done.\u001b[K\n",
            "remote: Total 173 (delta 23), reused 86 (delta 7), pack-reused 65\u001b[K\n",
            "Receiving objects: 100% (173/173), 74.07 MiB | 22.97 MiB/s, done.\n",
            "Resolving deltas: 100% (43/43), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/jomariya23156/face-recognition-with-liveness-web-login.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oa2EEiCDOPe2"
      },
      "outputs": [],
      "source": [
        "#!python /content/face-recognition-with-liveness-web-login/face_recognition_and_liveness/face_liveness_detection/train_model.py -d /content/drive/MyDrive/livenessnet/dataset -m /content/drive/MyDrive/livenessnet/face_detector -l /content/drive/MyDrive/livenessnet/le.pickle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "KYxifUX7Q-ox"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.chdir('/content/face-recognition-with-liveness-web-login/face_recognition_and_liveness/face_liveness_detection')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "FF2NxkxsQq9t"
      },
      "outputs": [],
      "source": [
        "# set the matplotlib backend, so figures can be saved in the background\n",
        "import matplotlib\n",
        "matplotlib.use('Agg') # Agg is used for writing files\n",
        "\n",
        "from livenessnet import LivenessNet # our model\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from imutils import paths\n",
        "\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import argparse\n",
        "import pickle\n",
        "import cv2\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "LLfvBPWNO4Af"
      },
      "outputs": [],
      "source": [
        "args = {}\n",
        "args[\"dataset\"] = '/content/drive/MyDrive/livenessnet/dataset'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "KfpBImi0Rj2S",
        "outputId": "f71e0337-7256-4791-9be7-07478bb28ad6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'fake: 627'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "f'fake: {len(list(paths.list_images(args[\"dataset\"]+\"/fake\")))}'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "jy5WU_pJRnMW",
        "outputId": "5d1a6e81-542a-4058-b555-56a889a892cb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'real: 799'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "f'real: {len(list(paths.list_images(args[\"dataset\"]+\"/real\")))}'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mhVhGA6WTHRc",
        "outputId": "fed93dc4-3772-4bc6-aeaa-1689c8faa193"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] loading images...\n"
          ]
        }
      ],
      "source": [
        "print(\"[INFO] loading images...\")\n",
        "imagePaths = list(paths.list_images(args[\"dataset\"]))\n",
        "#print(\"imagePaths: \", imagePaths)\n",
        "data = []\n",
        "labels = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rSo1C_1eRnqB",
        "outputId": "43a985b0-b616-46c1-f891-fb77557a9d51"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1426/1426 [05:49<00:00,  4.08it/s]\n",
            "100%|██████████| 1426/1426 [00:03<00:00, 401.48it/s]\n",
            "100%|██████████| 1426/1426 [00:03<00:00, 404.23it/s]\n",
            "100%|██████████| 1426/1426 [00:03<00:00, 382.23it/s]\n",
            "100%|██████████| 1426/1426 [00:03<00:00, 357.17it/s]\n",
            "100%|██████████| 1426/1426 [00:03<00:00, 400.14it/s]\n",
            "100%|██████████| 1426/1426 [00:03<00:00, 402.92it/s]\n",
            "100%|██████████| 1426/1426 [00:03<00:00, 367.59it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.69 s ± 175 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "%%timeit\n",
        "\n",
        "import tqdm\n",
        "for imagePath in tqdm.tqdm(imagePaths):\n",
        "    # extract the class label from the filename, load the image and\n",
        "    # resize it to be a fixed 32x32 pixels, ignoring aspect ratio\n",
        "    label = imagePath.split(os.path.sep)[-2]\n",
        "    # print(\": \", imagePath.split(os.path.sep)[1])\n",
        "    # print(\"os.path.sep: \", os.path.sep)\n",
        "    # print(\"label: \", label)\n",
        "    image = cv2.imread(imagePath)\n",
        "    image = cv2.resize(image, (64, 64))\n",
        "\n",
        "    # update the data and labels lists, respectively\n",
        "    data.append(image)\n",
        "    labels.append(label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ojCEfeX_S8i3",
        "outputId": "b6502885-70a8-45a2-f40c-d22ad67c05c9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "11408"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "len(data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "ALLEshpkR8BL"
      },
      "outputs": [],
      "source": [
        "# convert to ndarray and do feature scaling\n",
        "# tf works best with ndarray and it's super fast and efficient!\n",
        "data = np.array(data, dtype='float') / 255.0\n",
        "\n",
        "# encode the labels from (fake, real) to  (0,1)\n",
        "# and do one-hot encoding\n",
        "le = LabelEncoder()\n",
        "labels = le.fit_transform(labels)\n",
        "labels = tf.keras.utils.to_categorical(labels, 2) # one-hot encoding\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HAzaWqYiSwuC",
        "outputId": "71648124-7f70-4763-9759-f56aebf86d14"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "11408"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "len(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zx-bRfVMR7-R"
      },
      "outputs": [],
      "source": [
        "# train/test split\n",
        "# we go for traditional 80/20 split\n",
        "# Theoretically, we have small dataset we need test set to be a bit bigger\n",
        "# 75/25 or 70/30 split would be ideal, but from the trial and error\n",
        "# 80/20 gives a better result, so we go for it\n",
        "# Another thing to consider, since my dataset has only about 14 images of faces from card/solid image\n",
        "# so 80/20 has a higher chance that those images will be in training set rather than test set (and none on training set)\n",
        "# X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.20, random_state=42)\n",
        "# (X_train, X_test, y_train, y_test) = train_test_split(data, labels,\n",
        "#                                                   test_size=0.20, random_state=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "i6kAvFkNR76i"
      },
      "outputs": [],
      "source": [
        "aug = tf.keras.preprocessing.image.ImageDataGenerator(rotation_range=20,\n",
        "                         zoom_range=0.15,\n",
        "                         width_shift_range=0.2,\n",
        "                         height_shift_range=0.2,\n",
        "                         shear_range=0.15,\n",
        "                         horizontal_flip=True,\n",
        "                         fill_mode='nearest')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "AD8Hh5uvR73p"
      },
      "outputs": [],
      "source": [
        "# build a model\n",
        "# define hyperparameters\n",
        "INIT_LR = 1e-4 # initial learning rate\n",
        "BATCH_SIZE = 4\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SM9lN5FqR71T",
        "outputId": "cb80feb8-a94d-43fc-c7bb-b42e360348fa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] compiling model...\n"
          ]
        }
      ],
      "source": [
        "# we don't need early stopping here because we have a small dataset, there is no need to do so\n",
        "# initialize the optimizer and model\n",
        "print('[INFO] compiling model...')\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=INIT_LR)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "SfISu0fZR7x-"
      },
      "outputs": [],
      "source": [
        "model = LivenessNet.build(width=64, height=64, depth=3, classes=len(le.classes_))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "feGXahHIR7vB",
        "outputId": "ed99f026-864e-4c04-c321-719c93b7c3c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 64, 64, 16)        448       \n",
            "                                                                 \n",
            " batch_normalization (BatchN  (None, 64, 64, 16)       64        \n",
            " ormalization)                                                   \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 64, 64, 16)        2320      \n",
            "                                                                 \n",
            " batch_normalization_1 (Batc  (None, 64, 64, 16)       64        \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2D  (None, 32, 32, 16)       0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 32, 32, 16)        0         \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 32, 32, 32)        4640      \n",
            "                                                                 \n",
            " batch_normalization_2 (Batc  (None, 32, 32, 32)       128       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " conv2d_3 (Conv2D)           (None, 32, 32, 32)        9248      \n",
            "                                                                 \n",
            " batch_normalization_3 (Batc  (None, 32, 32, 32)       128       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPooling  (None, 16, 16, 32)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 16, 16, 32)        0         \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 8192)              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 64)                524352    \n",
            "                                                                 \n",
            " batch_normalization_4 (Batc  (None, 64)               256       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 64)                0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 2)                 130       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 541,778\n",
            "Trainable params: 541,458\n",
            "Non-trainable params: 320\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "8_ta__GRVGR4"
      },
      "outputs": [],
      "source": [
        "model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "qvFvWYwCVGHw"
      },
      "outputs": [],
      "source": [
        "# early stopping\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=4, restore_best_weights=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "UQqSD2WNn2Vu"
      },
      "outputs": [],
      "source": [
        "(X_train, X_test, y_train, y_test) = train_test_split(data, labels,\n",
        "                                                  test_size=0.20, random_state=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g80_RTw8VGCp",
        "outputId": "caff104d-9690-4da6-9d3f-137a32d87521"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] training model for 20 epochs...\n",
            "Epoch 1/20\n",
            "2281/2281 [==============================] - 43s 12ms/step - loss: 0.6629 - accuracy: 0.6796 - val_loss: 0.4114 - val_accuracy: 0.8611\n",
            "Epoch 2/20\n",
            "2281/2281 [==============================] - 28s 12ms/step - loss: 0.5186 - accuracy: 0.7643 - val_loss: 0.2529 - val_accuracy: 0.8988\n",
            "Epoch 3/20\n",
            "2281/2281 [==============================] - 28s 12ms/step - loss: 0.4645 - accuracy: 0.7966 - val_loss: 0.2912 - val_accuracy: 0.8865\n",
            "Epoch 4/20\n",
            "2281/2281 [==============================] - 28s 12ms/step - loss: 0.4390 - accuracy: 0.8083 - val_loss: 0.5338 - val_accuracy: 0.8019\n",
            "Epoch 5/20\n",
            "2281/2281 [==============================] - 28s 12ms/step - loss: 0.4210 - accuracy: 0.8205 - val_loss: 0.2203 - val_accuracy: 0.9229\n",
            "Epoch 6/20\n",
            "2281/2281 [==============================] - 28s 12ms/step - loss: 0.3940 - accuracy: 0.8319 - val_loss: 0.2723 - val_accuracy: 0.9268\n",
            "Epoch 7/20\n",
            "2281/2281 [==============================] - 28s 12ms/step - loss: 0.3754 - accuracy: 0.8385 - val_loss: 0.6870 - val_accuracy: 0.9075\n",
            "Epoch 8/20\n",
            "2281/2281 [==============================] - 28s 12ms/step - loss: 0.3891 - accuracy: 0.8321 - val_loss: 0.5410 - val_accuracy: 0.8944\n",
            "Epoch 9/20\n",
            "2281/2281 [==============================] - 29s 13ms/step - loss: 0.3646 - accuracy: 0.8511 - val_loss: 0.3725 - val_accuracy: 0.9229\n",
            "Epoch 10/20\n",
            "2281/2281 [==============================] - 28s 12ms/step - loss: 0.3620 - accuracy: 0.8490 - val_loss: 0.2677 - val_accuracy: 0.9207\n",
            "Epoch 11/20\n",
            "2281/2281 [==============================] - 28s 12ms/step - loss: 0.3562 - accuracy: 0.8547 - val_loss: 0.3055 - val_accuracy: 0.9036\n",
            "Epoch 12/20\n",
            "2281/2281 [==============================] - 28s 12ms/step - loss: 0.3465 - accuracy: 0.8533 - val_loss: 0.1718 - val_accuracy: 0.9514\n",
            "Epoch 13/20\n",
            "2281/2281 [==============================] - 28s 12ms/step - loss: 0.3386 - accuracy: 0.8585 - val_loss: 0.1310 - val_accuracy: 0.9496\n",
            "Epoch 14/20\n",
            "2281/2281 [==============================] - 28s 12ms/step - loss: 0.3469 - accuracy: 0.8567 - val_loss: 0.2496 - val_accuracy: 0.9281\n",
            "Epoch 15/20\n",
            "2281/2281 [==============================] - 28s 12ms/step - loss: 0.3452 - accuracy: 0.8544 - val_loss: 0.2225 - val_accuracy: 0.9036\n",
            "Epoch 16/20\n",
            "2281/2281 [==============================] - 30s 13ms/step - loss: 0.3271 - accuracy: 0.8665 - val_loss: 0.1587 - val_accuracy: 0.9461\n",
            "Epoch 17/20\n",
            "2281/2281 [==============================] - 28s 12ms/step - loss: 0.3139 - accuracy: 0.8724 - val_loss: 0.2013 - val_accuracy: 0.9592\n",
            "Epoch 18/20\n",
            "2281/2281 [==============================] - 27s 12ms/step - loss: 0.3251 - accuracy: 0.8643 - val_loss: 0.1226 - val_accuracy: 0.9575\n",
            "Epoch 19/20\n",
            "2281/2281 [==============================] - 28s 12ms/step - loss: 0.3205 - accuracy: 0.8686 - val_loss: 0.1395 - val_accuracy: 0.9549\n",
            "Epoch 20/20\n",
            "2281/2281 [==============================] - 27s 12ms/step - loss: 0.3090 - accuracy: 0.8795 - val_loss: 0.1861 - val_accuracy: 0.9207\n"
          ]
        }
      ],
      "source": [
        "# train the model\n",
        "EPOCHS = 20\n",
        "print(f'[INFO] training model for {EPOCHS} epochs...')\n",
        "history = model.fit(x=aug.flow(X_train, y_train, batch_size=BATCH_SIZE),\n",
        "                    validation_data=(X_test, y_test),\n",
        "                    steps_per_epoch=len(X_train) // BATCH_SIZE,\n",
        "                    epochs=EPOCHS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8voATupHXXbf",
        "outputId": "efe99d9c-69d9-4a9c-e768-72b3b7e47f94"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 3s 3s/step - loss: 0.1861 - accuracy: 0.9207\n",
            "\n",
            "Test Acc: 0.9206836223602295\n"
          ]
        }
      ],
      "source": [
        "result = model.evaluate(X_test, y_test, batch_size=10000)\n",
        "print('\\nTest Acc:', result[1])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ct9iIKDSXXYH",
        "outputId": "bddda771-bab3-420f-bdad-b38102fc6423"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "folder = f'/content/drive/MyDrive/livenessnet/model-E{EPOCHS}-Acc-{int(result[1]*100)}-t-{int(time.time())}'\n",
        "os.mkdir(folder)\n",
        "\n",
        "model.save(f\"{folder}/liveness.model\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "toaXS3l5pOO3"
      },
      "outputs": [],
      "source": [
        "model.save_weights(f'{folder}/livenessnet_weights.h5')\n",
        "f = open(f\"{folder}/encoded_faces.pickle\", \"wb\")\n",
        "f.write(pickle.dumps(le))\n",
        "f.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "NuesSVqJugpM"
      },
      "outputs": [],
      "source": [
        "# model.save_weights(f'{folder}/livenessnet_weights.h5')\n",
        "f = open(f\"{folder}/label_encoder.pickle\", \"wb\")\n",
        "f.write(pickle.dumps(le))\n",
        "f.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5M7WT2UFnSgV",
        "outputId": "f43d254b-da58-4383-c383-9afd01e05776"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _update_step_xla while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
          ]
        }
      ],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "mount_file_id": "1tEgiNSlGbdnRhGRRMx6GrpAGiH-Rd3wD",
      "authorship_tag": "ABX9TyN7j5L5L09NubgK1E3seSac",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}